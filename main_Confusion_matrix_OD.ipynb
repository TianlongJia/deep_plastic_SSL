{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix for CocoAPI\n",
    "\n",
    "The \"main_Evaluate_Object_Detection.ipynb\" uses the CocoApi to evaluate the model performance, e.g., AP50. But that code does not include the output of confusion matrix (e.g., TP, FP, and FN).\n",
    "\n",
    "This script provides the output of confusion matrix using **FiftyOne** tool\n",
    "\n",
    "FiftyOne’s implementation of COCO-style evaluation matches the reference implementation available via pycocotools (https://github.com/cocodataset/cocoapi).\n",
    "\n",
    "Refer to: \n",
    "https://docs.voxel51.com/integrations/coco.html#loading-coco-formatted-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install fiftyone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   For windows:\n",
    "#        pip3 install fiftyone --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load coco dataset (ground-truth)\n",
    "\n",
    "**Note**: \n",
    "\n",
    "(1) the folder/path of images in json file must contains \" / \" instead of \" \\ \" \n",
    "\n",
    "(2) In json file, \"image_id\" and the \"id\" for annotations must not start \"0\", otherwise the results will be not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1849/1849 [21.1s elapsed, 0s remaining, 135.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# A name for the dataset\n",
    "# name = \"my-dataset\"\n",
    "\n",
    "# GJO test dataset\n",
    "image_path = r\"U:\\AIMMW\\GJO\\Auto\\10%_images_test\\tiles_224\\images\"\n",
    "labels_path = r\"U:\\AIMMW\\GJO\\Auto\\10%_images_test\\tiles_224\\labels_coco\\SL_test_2620.json\"\n",
    "\n",
    "# Amsterdam test dataset\n",
    "# image_path = r\"U:\\AIMMW\\GJO\\Exp4\\Amsterdam\\images\"\n",
    "# labels_path = r\"U:\\AIMMW\\GJO\\Exp4\\Amsterdam\\labels_coco\\test.json\"\n",
    "\n",
    "# Groningen test dataset\n",
    "# image_path = r\"U:\\AIMMW\\GJO\\Exp4\\Groningen\\test\\tiles_224\\images\"\n",
    "# labels_path = r\"U:\\AIMMW\\GJO\\Exp4\\Groningen\\test\\tiles_224\\labels_coco\\SL_test_525.json\"\n",
    "\n",
    "# Vietnam test dataset\n",
    "# image_path = r\"U:\\AIMMW\\GJO\\Exp4\\Vietnam_3_class_V5\\test\\tiles_224\\images\"\n",
    "# labels_path = r\"U:\\AIMMW\\GJO\\Exp4\\Vietnam_3_class_V5\\test\\tiles_224\\labels_coco\\SL_test_1091.json\"\n",
    "\n",
    "\n",
    "# The type of the dataset being imported\n",
    "dataset_type = fo.types.COCODetectionDataset  # coco format\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=dataset_type,\n",
    "    data_path=image_path,\n",
    "    labels_path=labels_path,\n",
    "    # include_id=True,\n",
    "    # name=name,\n",
    ")\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first data in the dataset,\n",
    "# Carefully check the \"label\" name!\n",
    "# Note the coordinates of the bbox is scaled into [0,1]\n",
    "\n",
    "# print(dataset.first().detections.detections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load predictions\n",
    "\n",
    "When inferencing a model on a test dataset using Detectron2, it will output a prediction file named \"coco_instances_results.json\" with predicted bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['0', 'litter']\n",
      "1620\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.utils.coco as fouc\n",
    "import json\n",
    "\n",
    "#\n",
    "# Mock COCO predictions, where:\n",
    "# - `image_id` corresponds to the `coco_id` field of `coco_dataset`\n",
    "# - `category_id` corresponds to classes in `coco_dataset.default_classes`\n",
    "#\n",
    "# predictions = [\n",
    "#     {\"image_id\": 1, \"category_id\": 18, \"bbox\": [258, 41, 348, 243], \"score\": 0.87},\n",
    "#     {\"image_id\": 2, \"category_id\": 11, \"bbox\": [61, 22, 504, 609], \"score\": 0.95},\n",
    "# ]\n",
    "\n",
    "json_file= r\"U:\\AIMMW\\Paper 3 datasest and results\\checkpoints_(backup_2024.2.7)\\train_weights\\SSL_models_exp3\\Freeze_4\\SW_RN50_100e_FTAL\\SSL_train_2628\\test_results\\coco_instances_results.json\"\n",
    "\n",
    "with open(json_file, 'r') as fcc_file:\n",
    "    fcc_data = json.load(fcc_file)\n",
    "# print(fcc_data)\n",
    "    \n",
    "predictions = fcc_data\n",
    "\n",
    "\n",
    "# Add COCO predictions to `predictions` field of dataset\n",
    "classes = dataset.default_classes\n",
    "print(\"classes:\", classes)\n",
    "# classes = ['hold_space', 'litter',]\n",
    "# classes = ['airplane', 'apple', ...]\n",
    "fouc.add_coco_labels(dataset, \"predictions\", predictions, classes)\n",
    "\n",
    "# Verify that predictions were added to two images\n",
    "print(dataset.count(\"predictions\"))  # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849\n"
     ]
    }
   ],
   "source": [
    "print(dataset.count(\"detections\"))  # the number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first data (with prediction bbox) in the dataset ,\n",
    "# Carefully check the \"label\" name!\n",
    "# Note the coordinates of the predicted bbox is scaled into [0,1]\n",
    "\n",
    "# print(dataset.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dataset.first()\n",
    "# print(sample.detections.detections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output results \n",
    "\n",
    "by comparing ground-truth labels and predictions bbox\n",
    "\n",
    "**Note**: Predicted and ground truth objects are matched using a specified IoU threshold (default = 0.50). This threshold can be customized via the iou parameter. The IoU selection affects the calculation of TP, FN, are FP value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |███████████████| 1849/1849 [46.9s elapsed, 0s remaining, 67.0 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 1849/1849 [39.2s elapsed, 0s remaining, 63.6 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Evaluate the objects in the `predictions` field with respect to the\n",
    "# objects in the `ground_truth` field\n",
    "results = dataset.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"detections\",\n",
    "    iou=0.5,\n",
    "    method=\"coco\",   \n",
    "    eval_key=\"eval\",\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fiftyone.utils.eval.coco.COCODetectionResults at 0x25ee6ab6b20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mAP\n",
    "\n",
    "Note: this is the mAP at IoU=.50:.05:.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.mAP())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some statistics about the total TP/FP/FN counts\n",
    "print(\"TP: %d\" % dataset.sum(\"eval_tp\"))\n",
    "print(\"FP: %d\" % dataset.sum(\"eval_fp\"))\n",
    "print(\"FN: %d\" % dataset.sum(\"eval_fn\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, recall, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      litter       0.47      0.68      0.55      2620\n",
      "\n",
      "   micro avg       0.47      0.68      0.55      2620\n",
      "   macro avg       0.47      0.68      0.55      2620\n",
      "weighted avg       0.47      0.68      0.55      2620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = 1\n",
    "counts = dataset.count_values(\"detections.detections.label\")\n",
    "classes = sorted(counts, key=counts.get, reverse=True)[:classes]\n",
    "\n",
    "# Print a classification report for the top-10 classes\n",
    "results.print_report(classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = results.plot_pr_curves(classes=[\"litter\"])\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "\n",
    "# Perform evaluation, allowing objects to be matched between classes\n",
    "results = dataset.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=\"detections\",\n",
    "    method=\"coco\",\n",
    "    classwise=False,\n",
    ")\n",
    "\n",
    "# Generate a confusion matrix for the specified classes\n",
    "plot = results.plot_confusion_matrix(classes=[\"litter\"])\n",
    "plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

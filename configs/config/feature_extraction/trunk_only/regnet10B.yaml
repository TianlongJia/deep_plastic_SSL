# @package _global_
config:
  TRAINER:
    TASK_NAME: self_supervision_fsdp_task
  # Smaller batch sizes to deal with model memory consumption
  DATA:
    TRAIN:
      BATCHSIZE_PER_REPLICA: 16
    TEST:
      BATCHSIZE_PER_REPLICA: 16
  EXTRACT_FEATURES:
    # Adapt the chunk threshold to limit memory usage:
    # lower means lower memory usage but more write to disk
    CHUNK_THRESHOLD: 1000
  MODEL:
    FEATURE_EVAL_SETTINGS:
      EVAL_MODE_ON: True
      FREEZE_TRUNK_ONLY: True
      EXTRACT_TRUNK_FEATURES_ONLY: True
      SHOULD_FLATTEN_FEATS: False
      LINEAR_EVAL_FEAT_POOL_OPS_MAP: [
        ["res3", ["AdaptiveAvgPool2d", [[2, 2]]]],
        ["res4", ["AdaptiveAvgPool2d", [[2, 1]]]],
        ["avgpool", ["Identity", []]],
      ]
    TRUNK:
      NAME: regnet_fsdp
      REGNET:
        block_type: res_bottleneck_block
        depth: 27
        group_width: 1010
        w_0: 1744
        w_a: 620.83
        w_m: 2.52
        stage_checkpoints: [[2], [7], [9, 17], []]
    HEAD:
      PARAMS: []
    SYNC_BN_CONFIG:
      # No sync BN needed in eval mode
      CONVERT_BN_TO_SYNC_BN: False
      SYNC_BN_TYPE: pytorch
    FSDP_CONFIG:
      AUTO_WRAP_THRESHOLD: 100000000
      flatten_parameters: False
      compute_dtype: float32
      mixed_precision: False
      fp32_reduce_scatter: False
    AMP_PARAMS:
      # No AMP used in extraction mode
      USE_AMP: False
      AMP_TYPE: pytorch
    ACTIVATION_CHECKPOINTING:
      # With feature extraction, we do not need activation checkpointing
      # because the trunk is used in evaluation mode and does not need
      # to accumulate any gradient
      USE_ACTIVATION_CHECKPOINTING: False
